{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-29T07:27:22.942776Z",
     "start_time": "2025-04-29T07:27:18.220233Z"
    }
   },
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import base64\n",
    "import mimetypes\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from config import Config, CacheParameters"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:28:32.748935Z",
     "start_time": "2025-04-29T07:27:57.375591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "path = '../'+Config.vector_db_path\n",
    "print(path)\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "text_collection = client.get_collection(name=\"text_collection\")\n",
    "\n",
    "image_loader = ImageLoader()\n",
    "CLIP = OpenCLIPEmbeddingFunction()\n",
    "image_collection = client.get_collection(name=\"image_collection\",\n",
    "                                         embedding_function=CLIP,\n",
    "                                         data_loader=image_loader)\n"
   ],
   "id": "5f284fc45ca4102b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../mm_vdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piter\\PycharmProjects\\Multimodal_RAG_SoftServe\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:31:35.223433Z",
     "start_time": "2025-04-29T07:31:35.166028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_text = 'Airplanes'\n",
    "\n",
    "results = text_collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=10,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "results\n"
   ],
   "id": "9100177377ba67be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['Flight Paths Optimized',\n",
       "   'Training Mission',\n",
       "   'Where Drones Fly Free',\n",
       "   'Airfoils Automatically Optimized',\n",
       "   'No Jobs for Humans',\n",
       "   'Tracking Changes on Earth’s Surface',\n",
       "   'Drones Unleashed',\n",
       "   'Ukraine’s Homegrown Drones',\n",
       "   'Autonomous Drones Ready to Race',\n",
       "   'Meet the New Smart-Cities Champ']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['An AI system is helping aircraft avoid bad weather, restricted airspace, and clogged runways.\\nWhat’s new:Alaska Airlineswill route all its flights using a system fromAirspace Intelligencecalled Flyways.\\nHow it works:The system evaluates weather data, federal airspace closures, and the routes of all planned and active flights in the U.S. to find the most efficient paths for aircraft to reach their destinations.\\n• In a six-month trial last year, Alaska dispatchers accepted one-third of the system’s recommendations, shaving off an average of 5.3 minutes from 63 percent of flights. That saved an estimated 480,000 gallons of fuel, reducing the airline’s carbon dioxide emissions by 4,600 tons.\\n• The system constantly monitors each plane’s route while it’s in the air, sending color-coded alerts to human dispatchers. A red light suggests that a flight should be rerouted due to weather or safety issues. A green light flashes if the re-route is for fuel efficiency. A purple light means a flight needs to avoid restricted airspace.\\n• Alaska Airlines signed a multi-year agreement with Airspace Intelligence. Terms of the deal were not disclosed.\\nBehind the news:AI is making inroads into several areas of air transport.\\n• FedExpartnered with Reliable Robotics to build self-piloting Cessnas that carry cargo to remote areas.\\n• California startupMerlinplans to build a fleet of autonomous small planes to deliver cargo and fight fires.\\n• A number ofdrone delivery servicesare getting ready to take flight, pending permission from the U.S. Federal Aviation Administration.\\nWhy it matters:Commercial air travel gotwallopedby the pandemic. Streamlining operations may be necessary to revive it, according to theU.S. Travel Association.\\nWe’re thinking:Unlike cars and trucks, airplanes can’t easily go electric, so they’re stuck with fossil fuels for the foreseeable future. Cutting theircarbon emissionswill benefit everyone.\\n',\n",
       "   \"An experimental AI system is helping train the next generation of fighter pilots.What’s new:The U.S. Air Force is using deep learning to evaluate the progress of around 50 pilots in one of its training squadrons,Popular Sciencereported.Cloud-based data:Built by the California startup Crowdbotics, the system harnesses data generated in flight by F-15E airplanes (or simulations). Each aircraft records numerous data streams, such as air speed and position, multiple times per second. Instructors use the system’s output to tailor feedback to each student.\\n• The system grades trainees on their landings by monitoring the aircraft’s angle of approach, position on the runway, and remaining fuel. A plane that’s heavy with fuel may need to maintain a higher speed as it touches down than one that’s almost empty.\\n• It compares a trainee’s performance across different flights to evaluate improvement over time. It also compares trainees within a group, helping instructors to home in on areas for improvement.\\n• The project is funded bySmall Business Innovation Research, a competitive government program to nurture technologies that show potential for commercialization. The program will determine the project’s commercial viability within two years.\\nBehind the news:Several machine learning projects aim to improve pilot safety by taking advantage of the data produced by modern aircraft.\\n• Paladin AI, based in Montreal,analyzesflight and simulator data to help train commercial pilots by assessing their in-flight maneuvers, awareness of their surroundings, and ability to follow procedures.\\n• Aurabuilta computer vision system that monitors helicopter instrument displays to generate performance reports for helicopter pilots-in-training. Purportedly it cuts training time by as much as 10 percent.\\nWhy it matters:Training pilots is costly, time-consuming, and risky to both personnel and aircraft, which can cost tens of millions of dollars each. It’s also ongoing, as each type of aircraft requires unique instruction. AI can make training more effective, efficient, and safe. It can also allow instructors to focus on trainees who need the most attention.We’re thinking:The sky's the limit for machine learning in training applications.\\n\",\n",
       "   \"Autonomous aircraft in the United Kingdom are getting their own superhighway.What’s new:The UK governmentapprovedProject Skyway, a 165-mile system of interconnected drone-only flight routes. The airspace is scheduled to open by 2024.How it works:The routes, each just over six miles wide, will connect six medium-sized English cities including Cambridge, Coventry, Oxford, and Rugby. They avoid forested or ecologically sensitive areas, as well as major cities like London and Birmingham.\\n• A consortium of businesses will install a ground-based sensor network over the next two years to monitor air traffic along the Skyway. The sensors will supply information to help the drones navigate, removing the need for fliers to carry their own sensors.\\n• The sensors will also feed an air-traffic management system fromAltitude Angel, which will help the craft avoid midair collisions.\\n• The UK government isconsideringfuture extensions to coastal urban areas like Southampton and Ipswich.\\nBehind the news:Project Skyway is the largest proposed designated drone flight zone, but it’s not the only one.\\n• A European Union effort based in Irelandaimsto develop an air-traffic control system for autonomous aircraft including those used for deliveries, emergency response, agriculture, and personal transportation.\\n• In March 2021, authorities in Senegalgrantedapproval for drone startup Volansi to fly its aircraft outside of operators’ line of sight.\\n• The California city of Ontarioestablishedsafe flight corridors for drones built byAirspace Linkto fly between warehouses and logistics centers. The plan awaits approval by the United States Federal Aviation Administration.\\nYes, but:Although Skyway includes a collision-avoidance system, it’s not designed to prevent accidents during takeoff and landing, when they’re most common. Moreover, it's not yet clear whether the plan includes designated takeoff and landing sites. “The problem is what happens when you're 10 feet away from people,” one aerospace engineertoldthe BBC.Why it matters:Drones are restricted from flying in most places due to worries that they could interfere — or collide — with other aircraft. By giving them their own airspace, the UK is allowing drones to deliver on their potential without putting other aircraft at risk.We’re thinking:Figuring out how to operate drones safely has proven one of the most difficult aspects of deploying them in commercial applications. This project is a big step toward ironing out the regulatory bugs and also provides a relatively safe space to address technical issues.\\n\",\n",
       "   'Engineers who design aircraft, aqueducts, and other objects that interact with air and water use numerical simulations to test potential shapes, but they rely on trial and error to improve their designs. A neural simulator can optimize the shape itself.What’s new:Researchers at DeepMind devisedDifferentiable Learned Simulators, neural networks that learn to simulate physical processes, to help design surfaces that channel fluids in specific ways.Key insight:A popular way to design an object with certain physical properties is to evolve it using a numerical simulator: sample candidate designs, test their properties, keep the best design, tweak it randomly, and repeat. Here’s a faster, nonrandom alternative: Given parameters that define an object’s shape as a two- or three-dimensional mesh, a differentiable model can compute how it should change to better perform a task. Then it can use that information to adjust the object’s shape directly.How it works:Water and air can be modeled as systems of particles. The authors trainedMeshGraphNets, a type of graph neural network, to reproduce a prebuilt simulator’s output. The networks were trained to simulate the flow of particles around various shapes by predicting the next state given the previous state. The MeshGraphNets’ nodes represented particles, and their edges connected nearby particles.\\n• They trained one network to simulate the flow ofwater in two dimensionsand used it to optimize the shapes of containers and ramps. They trained another to simulatewater in three dimensionsand used it to design surfaces that directed an incoming stream in certain directions. They trained the third on the output of anaerodynamic solverand used it to design an airfoil — a cross-section of a wing — to reduce drag.\\n• Given a shape’s parameters, the trained networks predicted how the state would change over a set number of time steps by repeatedly predicting the next state from the current one. Then they evaluated the object based on a reward function. The reward functions for the 2D and 3D water tasks maximized the likelihood that particles would pass through a target region of simulated space. The reward function for the aerodynamic task minimized drag.\\n• To optimize a shape, the authors repeatedly backpropagated gradients from the reward function through the network (without changing it) to the shape, updating its parameters.\\nResults:Shapes designed using the authors’ approach outperformed those produced by thecross-entropy method(CEM), a technique that samples many designs and evolves them to maximize rewards. In the 2D water tasks, they achieved rewards 3.9 to 37.5 percent higher than shapes produced by CEM using the prebuilt simulator. In the aerodynamic task, they achieved results similar to those of a highly specializedsolver, producing drag coefficients between 0.01898 and 0.01919 compared to DAFoam’s 0.01902 (lower is better).We’re thinking:It’s not uncommon to train a neural network to mimic the output of a computation-intensive physics simulator. Using such a neural simulator not to run simulations but to optimize inputs according to the simulation’s outcome — that’s a fresh idea.\\n',\n",
       "   'AI is taking over the workplace. Will there be enough jobs left for people?\\nThe fear:Workers of all kinds are on the firing line as large language models, text-to-image generators, and hardware robots match their performance at a lower cost.\\nHorror stories:Automated systems are performing a wide range of tasks that previously required human labor.\\n• Voice-enabled language modelstake ordersat fast-food restaurants. Their mechanical counterpartscookfries.\\n• Large language models write articles for publications includingCNET,\\xa0Gizmodo, publications that share ownership with\\xa0Sports Illustrated, and outlets associated with the United Kingdom’s\\xa0Daily Mirror\\xa0and\\xa0Express.\\n• Image generators areproducingconcept art for game developer Blizzard Entertainment, and a synthetic imageappearedon the cover of a book published by Bloomsbury.\\n• Humanoidrobotsare moving bins in Amazon warehouses, while mechanical arms that shape sheet metalfabricateparts for airplanes.\\nCreeping pink slips:Workers are expressing anxiety about their prospects, and researchers believe the labor market is about to experience a seismic shift.\\n• 24 percent of U.S. workers worry AI will take over their jobs, a May survey by\\xa0CNBCfound.\\n• Hollywood writers and actors staged a protractedstrikepartly over concerns that generative AI would devalue their work.\\n• Investment bank Goldman Sachspredictedthat AI could put 300 million full-time jobs at risk.\\nFacing the fear:Each new wave of technology puts people out of work, and society has a responsibility to provide a safety net and training in new skills for people whose jobs become fully automated. In many cases, though, AI is not likely to replace workers — but workers who know how to use AI are likely to replace workers who don’t.\\n• The United States Bureau of Labor Statistics identified 11 occupations at risk of being automated — such as language translators and personal financial advisors — andfoundthat 9 of them grew between 2008 and 2018.\\n• Human jobs tend to involve many tasks, and while AI can do some of them, it’s poorly suited to others. An analysis of AI’s impact on jobs in the United Statesconcludedthat, for 80 percent of the workforce, large language models would affect at least 10 percent of tasks. This leaves room for AI to boost the productivity — and perhaps wages and even job security — of human workers.\\n• Technological advances typically create far more jobs than they destroy. An estimated 60 percent of U.S. jobs in 2018did not existin 1940. Looking forward, consider the likely explosion of machine learning engineers, data scientists, MLOps specialists, and roboticists.\\n',\n",
       "   'Computer vision systems are scanning satellite photos to track construction on the Earth’s surface — an exercise in behavior recognition on a global scale.What’s new:Space-based Machine Automated Recognition Technique (Smart) is a multi-phase competition organized by the United States government. So far, it has spurred teams to develop systems that track large-scale construction in sequential satellite images,Wiredreported.The challenge:Barren earth, dump trucks, and large cranes are common markers of construction sites. But they aren’t always present at the same time, and they may be found in other contexts — for instance, dump trucks travel on highways and large cranes sit idle between jobs. Moreover, different satellites have different imaging systems, orbits, schedules, and so on — a stumbling block for automated classification. In the first phase of the contest, from January 2021 through April 2022, competitors built models that correlate features that were present in the same location but not at the same time, regardless of the image source.How it works:The Intelligence Advanced Research Projects Activity (IARPA), a U.S. intelligence agency, organized the challenge.\\n• The agency provided 100,000 satellite images of 27 regions that range from fast-growing Dubai, where the population increased by nearly one million during that time period, to untouched parts of the Amazon rainforest. Roughly 13,000 images were labeled to indicate over 1,000 construction sites shot by multiple satellites at multiple points in time, as well as 500 non-construction activities that are similar to construction. Rather than dividing the dataset, which was made up of publicly available archives, into training and test sets, the agency split the annotations, withholding roughly labeled 300 construction sites for testing.\\n• The models were required to find areas of heavy construction, classify the current stage of construction, and alert analysts to specific changes. They were also required to identify features in areas of interest including thermal anomalies, soil permeability, and types of equipment present.\\n• The team at Kitware approached the problem by segmenting pixels according to the materials they depicted, then using a transformer model to track changes from one image to the next. In contrast, Accenture Federal Services trained its model on unlabeled data to recognize similar clusters of pixels.\\nResults:Judges evaluated contestants based on how they approached the problem and how well their models performed. The jury came from institutions including NASA’s Goddard Space Flight Center, U.S. Geological Survey, and academic labs.\\n• The judges advanced six teams to the second phase: Accenture Federal Services, Applied Research Associates, \\xa0BlackSky, Intelligent Automation (now part of Blue Halo), Kitware, and Systems & Technology Research.\\n• In the second phase, teams will adapt their construction-recognition models to different change-over-time tasks such as detecting crop growth. It will continue through 2023\\n• The third phase, beginning in 2024, will challenge participants to build systems that generalize to different types of land use.\\n• Teams are allowed to use the systems they develop for commercial purposes, and all datasets are publicly available.\\nBehind the news:Satellite imagery is a major target of development in computer vision. Various teams are tracking the impact ofclimate change, predicting volcaniceruptions, and watching China’s post-Covid economyrebound.Why it matters:Photos taken from orbit are akeyresource for intelligence agencies. Yet the ability to see changes on Earth’s surface is a potential game changer in fields as diverse as agriculture, logistics, and disaster relief. It’s impractical for human analysts to comb the flood of images from more than 150 satellites thatobserveEarth from orbit. By automating the process, machine learning opens huge opportunities beyond Smart’s focus on national security.We’re thinking:Large-scale events on Earth are of interest to all of the planet’s inhabitants. We’re glad to see that the contestants will be able to use the models they build, and we call on them to use their work to help people worldwide.\\n',\n",
       "   'U.S. regulators for the first time allowed commercial operators of autonomous aerial vehicles to fly out of operators’ sight.What’s new:The U.S. Federal Aviation Administration generally requires people on the ground to keep an eye on drones, but itauthorizeddrone maker American Robotics to fly without requirement.How it works:The company’s 20-pound quadcopters travel predetermined paths and automatically avoid collisions with birds, aircraft, and other obstacles.\\n• When they’re not in the air, the drones charge their battery in a weatherproof launch pad, which also houses computing horsepower for navigation.\\n• Anacoustic sensing systemrecognizes the presence and direction of airborne objects. It commands the robot to descend if it detects an object flying within a two-mile perimeter.\\n• A human technician must run through a safety checklist and inspect drones before takeoff, but these functions can be performed remotely. Flights are limited to daylight hours, altitudes under 400 feet, and limited areas in Kansas, Massachusetts, and Nevada, according toThe Verge.\\nBehind the news:Companies can apply to the FAA for a waiver of theline-of-sight rule. American Robotics became the first company to receive one after four years of testing.\\n• The agency recently issuedrulesgoverning flights in populated areas and at night — a step toward a full regulatory framework for drone delivery services.\\n• Last August, the agency grantedAmazonandWinglimited permission to deliver packages via drones.\\n• The U.S. approach to drone regulations is relatively permissive.Most countriesrestrict flights to an operator’s line of sight.\\nWhy it matters:The ability to operate without a human in visual contact is a critical step to making drone flights easier to manage and more economical to operate.We’re thinking:Andrew used to work with Pieter Abbeel, Adam Coates, and others on reinforcement learning to get autonomous helicopters to flystunts. He crashed quite a few copters in the process! (Safely, of course, in empty fields.) With drones now flying out of an operator’s line of sight, it’s more important than ever to subject their hardware and software to robust safety testing and verification.\\n',\n",
       "   'The war in Ukraine has spurred a new domestic industry.\\nWhat’s new:Hundreds of drone companies have sprung up in Ukraine since Russian forces invaded the country early last year,The Washington Postreported.\\nHow it works:Ukrainian drone startups are developing air- and sea-borne robots, which the country’s military use to monitor enemy positions, guide artillery strikes, and drop bombs, sometimes on Russian territory.\\n• Quadcopters built by Twist Robotics use AI-powered target tracking to remain locked onto targets even if the operator loses radio contact. Air and naval drones from Warbirds have similar capabilities.\\n• Working in an active war zone gives local drone makers advantages over their foreign counterparts. For instance, Ukrainian authorities give domestic firms access to captured Russian jamming technology so that they can develop countermeasures. Similarly, the companies acquire huge amounts of real-world data from the front lines, such as images of tanks or landmines in a variety of settings, that can be used to train their systems. They also receive immediate feedback on how their machines perform on the battlefield.\\n• Foreign companies are angling to get involved — partly to gain access to the same data. Canada-based Draganfly and U.S.-based BRINC are actively developing drones in Ukraine. German defense-AI company Helsing and U.S. data analytics firm Palantir also maintain offices there.\\nRussia responds:In recent months, Russia has stepped up attacks by Russian-made Lancet fliers that explode upon crashing into their targets. Recent units appear to contain Nvidia Jetson TX2 computers, which could drive AI-powered guidance or targeting,Forbesreported. Russian state news denied that its drones use AI.\\nBehind the news:Other countries are also gearing up for drone warfare.\\n• A U.S. Navy group called Task Force 59 recentlytesteda system, built from off-the-shelf components, that identifies threats based on data from drones, other air vessels, surface ships, and submarines.\\n• The Israel Defense Forces reportedlydeployedan AI system that selects targets for air strikes. A separate system then calculates munition loads, schedules strikes, and assigns targets to drones and crewed aircraft.\\n• Taiwanlauncheda major program to build its own drones.\\nWhy it matters:Drones rapidly have become a battlefield staple, and their offensive capabilities are growing. Governments around the world are paying close attention for lessons to be learned — as are, no doubt, insurgent forces, paramilitary groups, and drug cartels.We’re thinking:We stand with the brave Ukrainian soldiers as they defend their country against an adversary with a much larger air force. War is tragic and ugly. We wish that no one used AI-enabled weapons. But the reality is that peaceful and democratic nations do, if only to defend themselves against adversaries who do the same. We are heartened by recentagreementsto limit development of fully autonomous weapons, and we support the United Nations’proposalto ban them entirely.\\n',\n",
       "   'Pilots in drone races fly souped-up quadcopters around an obstacle course at 120 miles per hour. But soon they may be out of a job, as race organizers try to spice things up with drones controlled by AI.What’s new:The Drone Racing League, which stages contests to promote this so-called sport of the future, recently unveiled an autonomous flier called RacerAI. The new drone includes Nvidia’s Jetson AGX Xavier inference engine, four stereoscopic cameras, and propellers that deliver 20 pounds of thrust.What’s happening:RacerAI serves as the platform for AI models built by teams competing in AlphaPilot, a competition sponsored by the DRL and Lockheed Martin.\\n• 420 teams entered and tested their models on a simulated track.\\n• Virtual trials whittled the teams down to nine, which will compete in four races throughout fall 2019.\\n• Team USRG from Kaist University in South Korea won the first race on October 8. The second is scheduled for November 2 in Washington D.C.\\n• The series winner will take a $1 million prize. In early 2020, that model will face a top-rated human pilot for an additional $250,000 purse.\\nBehind the news:Drone Racing League pilots use standardized drones built and maintained by theleague, and train on the same simulator used to train RacerAI. Races are typically a mile long and take place in event spaces across the U.S. and Europe.Why it matters:Drone racing is fun and games, but the skills learned by autonomous racing models could be transferable to real-world applications like automated delivery.We’re thinking:A recent DRLvideoshows that current models have a way to go before they graduate from passing through rings to making high-speed maneuvers. Human pilots still have a significant edge — for now.\\n',\n",
       "   'Chinese researchers for the first time swept a competition to develop AI systems that monitor urban traffic.What’s new:Chinese universities and companies won first and second place in all five categories of the2021 AI City Challenge, beating hundreds of competitors from 38 nations. U.S. teams dominated the competition in its first three years, but Chinese contestants started overtaking them last year.What happened:305 teams entered at least one of the competition’sfive tracks. All teams used the same training and testing data for each track. Here’s a summary of the challenges and winners:\\n• Counting the number of vehicles turning left, turning right, or going straight through an intersection. Winner: Baidu/Sun Yat-sen University.\\n• Tracking individual vehicles across multiple cameras. Winner: Alibaba.\\n• Tracking multiple vehicles across multiple cameras scattered around a city. Winner: Alibaba/University of China Academy of Sciences.\\n• Detecting car crashes, stalled vehicles, and other traffic anomalies. Winner: Baidu/Shenzhen Institute of Advanced Technology.\\n• Identifying vehicles using natural-language descriptions (a new challenge for this year’s contest). Winner: Alibaba/University of Technology Sydney/Zhejiang University.\\nBehind the news:Nvidia, QCraft, and several universities launched the AI City Challenge in 2017 to spur the development of smart city technology.Why it matters:This competition is the latest example of China’s rising profile in AI. The Chinese government has funded hundreds ofSmart Cityprograms. In contrast, U.S. funding for urban AI initiatives has been limited to a few one-off grants or competitions.We’re thinking:Smart-city technology could make urban living more pleasant and productive, yet it also carries a risk of invasive surveillance. We call on regulators and researchers who work on such projects worldwide to lead a global debate on appropriate standards of privacy and to design their systems that protect privacy from the ground up.\\n']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'issue_id': 98,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-98/'},\n",
       "   {'issue_id': 142,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-142/'},\n",
       "   {'issue_id': 156,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-156/'},\n",
       "   {'issue_id': 149,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-149/'},\n",
       "   {'issue_id': 220,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-220/'},\n",
       "   {'issue_id': 152,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-152/'},\n",
       "   {'issue_id': 76, 'url': 'https://www.deeplearning.ai/the-batch/issue-76/'},\n",
       "   {'issue_id': 208,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-208/'},\n",
       "   {'issue_id': 9, 'url': 'https://www.deeplearning.ai/the-batch/issue-9/'},\n",
       "   {'issue_id': 102,\n",
       "    'url': 'https://www.deeplearning.ai/the-batch/issue-102/'}]],\n",
       " 'distances': [[1.251474142074585,\n",
       "   1.3054754734039307,\n",
       "   1.40058434009552,\n",
       "   1.4669532775878906,\n",
       "   1.4954074931311592,\n",
       "   1.5065743923187256,\n",
       "   1.508230209350586,\n",
       "   1.517758846282959,\n",
       "   1.5477057695388794,\n",
       "   1.5684350728988647]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:36:41.715221Z",
     "start_time": "2025-04-29T07:36:41.699578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_texts = []\n",
    "counter = 1\n",
    "for doc, distance, metadatas, title in zip(results['documents'][0], results['distances'][0], results['metadatas'][0], results['ids'][0]):\n",
    "    if 10 is None or distance <= 10:\n",
    "        full_doc_text = f'Title: {title}\\nURL: {metadatas.get('url', '')}\\nText: {doc}'\n",
    "        filtered_texts.append(full_doc_text)\n",
    "    counter += 1\n"
   ],
   "id": "d56e814740d37562",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "<>:4: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "C:\\Users\\Piter\\AppData\\Local\\Temp\\ipykernel_18940\\4115483523.py:4: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "  if 10 is None or distance <= 10 and counter==1:\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T07:36:42.737571Z",
     "start_time": "2025-04-29T07:36:42.733152Z"
    }
   },
   "cell_type": "code",
   "source": "doc",
   "id": "f82dce73814ef8f8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese researchers for the first time swept a competition to develop AI systems that monitor urban traffic.What’s new:Chinese universities and companies won first and second place in all five categories of the2021 AI City Challenge, beating hundreds of competitors from 38 nations. U.S. teams dominated the competition in its first three years, but Chinese contestants started overtaking them last year.What happened:305 teams entered at least one of the competition’sfive tracks. All teams used the same training and testing data for each track. Here’s a summary of the challenges and winners:\\n• Counting the number of vehicles turning left, turning right, or going straight through an intersection. Winner: Baidu/Sun Yat-sen University.\\n• Tracking individual vehicles across multiple cameras. Winner: Alibaba.\\n• Tracking multiple vehicles across multiple cameras scattered around a city. Winner: Alibaba/University of China Academy of Sciences.\\n• Detecting car crashes, stalled vehicles, and other traffic anomalies. Winner: Baidu/Shenzhen Institute of Advanced Technology.\\n• Identifying vehicles using natural-language descriptions (a new challenge for this year’s contest). Winner: Alibaba/University of Technology Sydney/Zhejiang University.\\nBehind the news:Nvidia, QCraft, and several universities launched the AI City Challenge in 2017 to spur the development of smart city technology.Why it matters:This competition is the latest example of China’s rising profile in AI. The Chinese government has funded hundreds ofSmart Cityprograms. In contrast, U.S. funding for urban AI initiatives has been limited to a few one-off grants or competitions.We’re thinking:Smart-city technology could make urban living more pleasant and productive, yet it also carries a risk of invasive surveillance. We call on regulators and researchers who work on such projects worldwide to lead a global debate on appropriate standards of privacy and to design their systems that protect privacy from the ground up.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "431f05b2f3c15ce0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
